<p align="center">
  <img src="Ana-lattex_Logo.png" alt="Ana-LatteX Logo" width="400" height="400"/>
</p>

# Super Café ETL Pipeline

**This repository is for the Super Café Project by Ana-LatteX**  
*Ana-LatteX — Automated ETL & BI*

---

## Summary
Fully automated ETL pipeline (local & AWS): ingest daily branch CSVs, clean and normalize transaction and order data, load into PostgreSQL (local) or Redshift (AWS). Visualize trends and insights with Grafana dashboards for branch and product performance.

---

## Table of Contents
1. [Overview of the Project](#overview-of-the-project)  
2. [Repository Contents](#repository-contents)  
3. [Simplified Explanation for Stakeholders](#simplified-explanation-for-stakeholders)  
4. [System Architecture & Data Flow](#system-architecture--data-flow)  
5. [Pipeline Workflow](#pipeline-workflow)  
6. [Getting Started for Developers](#getting-started-for-developers)  
7. [Deployment Instructions](#deployment-instructions)  
8. [Configuration and Secrets Management](#configuration-and-secrets-management)  
9. [Monitoring & Analytics](#monitoring--analytics)  
10. [Testing & Quality Checks](#testing--quality-checks)  
11. [Future Improvements](#future-improvements)  
12. [Team Contacts](#team-contacts)  

---

## Overview of the Project
This ETL pipeline automates the ingestion, transformation, and storage of daily transaction data from multiple café branches. Key benefits:

- Centralized storage of branch transactions  
- Standardized data format for analytics  
- Real-time business insights via Grafana  
- Scalable architecture supporting AWS deployment and local testing  

---

## Repository Contents

```.
├── AWS/                  # AWS deployment scripts
├── Database/             # Database schema and scripts
├── Sample Data 2/        # Sample CSV files
├── data/                 # Raw CSV files
├── doc/                  # Documentation
├── etl/                  # ETL pipeline code
├── src/                  # Source code
├── test/                 # Unit tests
├── .Ana-lattex_Logo.png  # Team logo
├── .env                  # Environment variables
├── .gitignore            # Git ignore rules
├── README.md             # Project documentation        # Team logo in PDF
└── ana-lattex.md         # Additional documentation
```


---

## Simplified Explanation for Stakeholders
- Each branch produces a daily CSV file.  
- Current reporting is manual and per-branch.  
- The Ana-LatteX ETL pipeline automates ingestion and loading into a central database.  
- Grafana dashboards provide branch- and product-level analytics, enabling faster decision-making.  

**High-level vision:**  
`Daily CSV → Automated ETL → Central Database → Grafana BI Dashboards`

<p align="center">
  <img src="Local etl pipeline architecture.png" alt="Local etl pipeline architecture" width="400" height="400"/>
</p>

---

## System Architecture & Data Flow

**ETL Flow Overview:**





# ana-lattex-de-x6-generation

Project background: 
Our Cafe order application was a success, the client now wants to facilitate their unprecedented growth and expansion to hundreds of outlets.
The client wants to target new and returning customers to understand which of their products are best sellers.

Client requirements:
Current set up - 
*Each branch creates a CSV file of transactions daily at 8pm that are uploaded to software in back-office computers.
*To pull data for reporting, they have to manually collect the data from each location to collate, which is time consuming and it is difficult to colllect meaningful data for the company. 
*They would like a platform that will upload all the data to a centralised online location to allow for easier data manipulation and will help them identify trends to maxise revenue streams.

Consult results:
To resolve this data issue we will build a fully scalable ETL pipeline to handle large volumes of tranactional information.
This pipeline will collect all the transaction data generated by each individual cafe, and place it into a PostSQL database.
This will allow for easy access to relevant data to process, store and analyse.
New set up - 
* Each night a CSV for each branch will be uploaded to the cloud.
* The pipeline will read each file and Extract, Transform and Load the data.
* Data will be stored in a data warehouse.
* Data Analytics software will be used to create Business Intelligence analytics for the client.
* Application monitoring software used to produce operational metrics (i.e. system errors, up-time, etc).

how to run the app:
locally -

How to run the Super Cafe app (file based) Prerequisites - Python 3 this can be downloaded from https://www.python.org/

Download the project: If using Git clone the repository by using the code below;

git Clone (https://github.com/DE-X6-LM/ana-lattex-de-x6-generation.git)

If downloaded as a .zip file, extract this and navigate to the ana-lattex-de-x6-generation directory.

It is recommended to create a virtual envirnoment to help manage project dependencies

To create a virtual envirnoment:

python -m venv venv

Then activate this: Windows: .\venv\Scripts\activate or .\venv\Scripts\activate.ps1

MacOS/Linux: source venv/bin/activate

Once active your command prompt will now show (venv) at the beginning.

local (PostgresSQL) database -

pip install will not be needed for exteral libraries, however this would be useful for when using a database and scalability.

Running the application:

Ensure virtual envirnoment is active
Navigate to root directory of Super Cafe app:
cd ana-lattex-de-x6-generation

Run main app file:
main.py

This will show the main menu and you can use the on screen prompt to navigate the menus and manage food items, orders and couriers.

Data persistence:
All changes made in the apps menus will be automatically saved to the CSV files in the data directory. Dependant on the connection chosen, this is also handled by the local Postgres database and the AWS Redshift cloud database as it will allow for larger volumes of data to be saved without affecting the app.

How to run any unit tests:



Week 1 Sprint:
Scrum master Prajakta 

Week 2 Sprint:
Scrum master Rahidur

Week 3 Sprint:
Scrum master Kimira

Week 4 Sprint:
Scrum master Michael

Week 5 Sprint 
Scrum master TBC


